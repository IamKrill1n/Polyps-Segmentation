{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":30892,"databundleVersionId":2715462,"sourceType":"competition"},{"sourceId":7187809,"sourceType":"datasetVersion","datasetId":4116340},{"sourceId":9993523,"sourceType":"datasetVersion","datasetId":6150706},{"sourceId":9998059,"sourceType":"datasetVersion","datasetId":6152584},{"sourceId":10001462,"sourceType":"datasetVersion","datasetId":6156191},{"sourceId":172466,"sourceType":"modelInstanceVersion","modelInstanceId":145357,"modelId":167918}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install -q segmentation-models-pytorch\n%pip install -q timm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:48:00.252972Z","iopub.execute_input":"2024-11-24T17:48:00.253264Z","iopub.status.idle":"2024-11-24T17:48:23.085499Z","shell.execute_reply.started":"2024-11-24T17:48:00.253235Z","shell.execute_reply":"2024-11-24T17:48:23.084375Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport cv2\nfrom torchvision.io import read_image\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, random_split, DataLoader, Subset\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport os\n\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport segmentation_models_pytorch as smp\nimport wandb\nimport torch.optim as optim\nfrom torch.amp import autocast, GradScaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:48:23.087304Z","iopub.execute_input":"2024-11-24T17:48:23.087556Z","iopub.status.idle":"2024-11-24T17:48:30.855438Z","shell.execute_reply.started":"2024-11-24T17:48:23.087531Z","shell.execute_reply":"2024-11-24T17:48:30.854754Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.17). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:48:30.856704Z","iopub.execute_input":"2024-11-24T17:48:30.856940Z","iopub.status.idle":"2024-11-24T17:48:30.882389Z","shell.execute_reply.started":"2024-11-24T17:48:30.856916Z","shell.execute_reply":"2024-11-24T17:48:30.881585Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, img_dir=\"path/to/data\", resize = None, transform=None):\n        self.img_dir = img_dir\n        self.transform = transform\n        self.resize = resize\n        self.images = os.listdir(self.img_dir)\n\n    def __len__(self):\n        return len(self.images)\n    \n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.images[idx])\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        height, width, channels = image.shape\n        image = cv2.resize(image, self.resize, interpolation=cv2.INTER_AREA)\n        name =  os.path.basename(img_path)\n        if self.transform:\n            transformed = self.transform(image=image)\n            image = transformed[\"image\"]\n        return image, height, width, name[:-5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:48:30.884367Z","iopub.execute_input":"2024-11-24T17:48:30.884644Z","iopub.status.idle":"2024-11-24T17:48:30.893975Z","shell.execute_reply.started":"2024-11-24T17:48:30.884619Z","shell.execute_reply":"2024-11-24T17:48:30.893330Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"img_resize = (256, 256)\nbatch_size = 16\nlearning_rate = 0.0000625\nalpha = 0.5  # Weight of cross entropy loss \nclass_weights = [0.05, 0.25, 0.70]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:48:30.895005Z","iopub.execute_input":"2024-11-24T17:48:30.895334Z","iopub.status.idle":"2024-11-24T17:48:30.904376Z","shell.execute_reply.started":"2024-11-24T17:48:30.895298Z","shell.execute_reply":"2024-11-24T17:48:30.903656Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"val_transformation = A.Compose([\n    A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\nclass UnNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n        \n    def __call__(self, tensor):\n        \"\"\"\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        Returns:\n            Tensor: Normalized image.\n        \"\"\"\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.mul_(s).add_(m)\n            # The normalize code -> t.sub_(m).div_(s)\n        return tensor\n    \nunorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:48:30.905260Z","iopub.execute_input":"2024-11-24T17:48:30.905477Z","iopub.status.idle":"2024-11-24T17:48:30.917259Z","shell.execute_reply.started":"2024-11-24T17:48:30.905455Z","shell.execute_reply":"2024-11-24T17:48:30.916623Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"test_dataset = TestDataset(img_dir=\"/kaggle/input/bkai-igh-neopolyp/test/test\", resize=img_resize, transform=val_transformation)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T18:20:18.194859Z","iopub.execute_input":"2024-11-24T18:20:18.195483Z","iopub.status.idle":"2024-11-24T18:20:18.203134Z","shell.execute_reply.started":"2024-11-24T18:20:18.195447Z","shell.execute_reply":"2024-11-24T18:20:18.202320Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n    \n    \nclass DownBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DownBlock, self).__init__()\n        self.double_conv = DoubleConv(in_channels, out_channels)\n        self.down_sample = nn.MaxPool2d(2)\n\n    def forward(self, x):\n        skip_out = self.double_conv(x)\n        down_out = self.down_sample(skip_out)\n        return (down_out, skip_out)\n\n    \nclass UpBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, up_sample_mode):\n        super(UpBlock, self).__init__()\n        if up_sample_mode == 'conv_transpose':\n            if out_channels*4 == in_channels:\n                self.up_sample = nn.ConvTranspose2d(in_channels-out_channels*2, in_channels-out_channels*2, kernel_size=2, stride=2) \n            else:\n                self.up_sample = nn.ConvTranspose2d(in_channels-out_channels, in_channels-out_channels, kernel_size=2, stride=2)\n        else:\n            self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.double_conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, down_input, skip_input):\n#         print(\"down\", down_input.shape)\n#         print(\"skip\", skip_input.shape)\n        x = self.up_sample(down_input)\n#         print(\"x\",x.shape)\n        x = torch.cat([x, skip_input], dim=1)\n        return self.double_conv(x)\n# ResUnet\nclass PolypModel(nn.Module):\n    def __init__(self, out_classes=3, up_sample_mode='conv_transpose'):\n        super().__init__()\n        self.out_classes = out_classes\n        self.backbone = timm.create_model(\"resnet152\", pretrained=True, features_only=True)\n#         self.down_conv1 = DownBlock(3, 64)\n        self.down_conv1 = DownBlock(64, 128)\n        self.down_conv2 = DownBlock(256, 512)\n        self.down_conv3 = DownBlock(512, 1024)\n        self.down_conv4 = DownBlock(1024, 2048)\n        self.up_sample_mode = up_sample_mode\n        self.block_neck = DoubleConv(2048, 1024)\n        self.block_up1 = UpBlock(1024+1024, 512,self.up_sample_mode)\n        self.block_up2 = UpBlock(512+512, 256,self.up_sample_mode)\n        self.block_up3 = UpBlock(256+256, 128,self.up_sample_mode)\n        self.block_up4 = UpBlock(128+64, 64,self.up_sample_mode)\n        self.conv_last = nn.Conv2d(64, out_classes, kernel_size=1)\n        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\")\n        self.final_activation = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x1, x2, x3, x4, x5 = self.backbone(x)\n        x = self.block_neck(x5) # x (B, 1024, 8, 8)\n        x = self.block_up1(x, x4)\n        x = self.block_up2(x, x3)\n        x = self.block_up3(x, x2)\n        x = self.block_up4(x, x1 )\n        x = self.conv_last(x)\n        x = self.upsample(x)\n        x = self.final_activation(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:48:30.973163Z","iopub.execute_input":"2024-11-24T17:48:30.973693Z","iopub.status.idle":"2024-11-24T17:48:30.986894Z","shell.execute_reply.started":"2024-11-24T17:48:30.973656Z","shell.execute_reply":"2024-11-24T17:48:30.986114Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"models = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T18:14:37.424418Z","iopub.execute_input":"2024-11-24T18:14:37.425191Z","iopub.status.idle":"2024-11-24T18:14:37.428915Z","shell.execute_reply.started":"2024-11-24T18:14:37.425154Z","shell.execute_reply":"2024-11-24T18:14:37.427981Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"model = PolypModel()\ncheckpoint = torch.load('/kaggle/input/model-polyp/colorization_model.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel = model.to(device)\nmodel.eval()\nmodels.append(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:48:31.003332Z","iopub.execute_input":"2024-11-24T17:48:31.003975Z","iopub.status.idle":"2024-11-24T17:48:57.956706Z","shell.execute_reply.started":"2024-11-24T17:48:31.003950Z","shell.execute_reply":"2024-11-24T17:48:57.955718Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38c0472ed499400aa6f8578e15795b44"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_30/4033220648.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load('/kaggle/input/model-polyp/colorization_model.pth')\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"model = smp.UnetPlusPlus(\n    encoder_name=\"timm-resnest200e\",        \n    # encoder_weights=\"imagenet\",     \n    in_channels=3,                  \n    classes=3     \n)\ncheckpoint = torch.load('/kaggle/input/unetplusplus-resnest/model.pth')\nmodel.load_state_dict(checkpoint['model'])\nmodel = model.to(device)\nmodel.eval()\nmodels.append(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T18:14:40.784524Z","iopub.execute_input":"2024-11-24T18:14:40.785239Z","iopub.status.idle":"2024-11-24T18:14:44.104180Z","shell.execute_reply.started":"2024-11-24T18:14:40.785206Z","shell.execute_reply":"2024-11-24T18:14:44.103232Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/1739671966.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load('/kaggle/input/unetplusplus-resnest/model.pth')\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# model = smp.UnetPlusPlus(\n#     encoder_name=\"resnet50\",        \n#     # encoder_weights=\"imagenet\",     \n#     in_channels=3,                  \n#     classes=3     \n# )\n# checkpoint = torch.load('/kaggle/input/unet/pytorch/default/2/resnet50.pth')\n# model.load_state_dict(checkpoint['model'])\n# model = model.to(device)\n# model.eval()\n# models.append(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:59:24.884636Z","iopub.execute_input":"2024-11-24T17:59:24.885261Z","iopub.status.idle":"2024-11-24T17:59:29.993822Z","shell.execute_reply.started":"2024-11-24T17:59:24.885227Z","shell.execute_reply":"2024-11-24T17:59:29.993122Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/1645129226.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load('/kaggle/input/unet/pytorch/default/2/resnet50.pth')\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"model = smp.UnetPlusPlus(\n    encoder_name=\"resnet34\",        \n    # encoder_weights=\"imagenet\",     \n    in_channels=3,                  \n    classes=3     \n)\ncheckpoint = torch.load('/kaggle/input/unet/pytorch/default/2/model.pth')\nmodel.load_state_dict(checkpoint['model'])\nmodel = model.to(device)\nmodel.eval()\nmodels.append(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T18:14:44.585454Z","iopub.execute_input":"2024-11-24T18:14:44.586096Z","iopub.status.idle":"2024-11-24T18:14:45.493009Z","shell.execute_reply.started":"2024-11-24T18:14:44.586037Z","shell.execute_reply":"2024-11-24T18:14:45.492019Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/810136401.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load('/kaggle/input/unet/pytorch/default/2/model.pth')\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"model = smp.UnetPlusPlus(\n    encoder_name=\"resnet152\",        \n    # encoder_weights=\"imagenet\",     \n    in_channels=3,                  \n    classes=3     \n)\ncheckpoint = torch.load('/kaggle/input/unetplusplus-resnet152/model.pth')\nmodel.load_state_dict(checkpoint['model'])\nmodel = model.to(device)\nmodel.eval()\nmodels.append(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T18:15:56.016698Z","iopub.execute_input":"2024-11-24T18:15:56.017042Z","iopub.status.idle":"2024-11-24T18:16:05.171997Z","shell.execute_reply.started":"2024-11-24T18:15:56.017011Z","shell.execute_reply":"2024-11-24T18:16:05.171024Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2997363928.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load('/kaggle/input/unetplusplus-resnet152/model.pth')\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"import torch\nimport segmentation_models_pytorch as smp\n\n# Set device\n\n# Number of folds\nn_splits = 5  # Adjust based on your cross-validation\n\n# Model parameters (should match those used during training)\nmodel_params = {\n    'encoder_name': 'resnet34',        # Same encoder as in training\n    'encoder_weights': None,           # No pretraining since weights are loaded\n    'in_channels': 3,\n    'classes': 3                       # Number of segmentation classes\n}\n\n# Load models from each fold\nfor fold in range(n_splits):\n    model = smp.UnetPlusPlus(**model_params)\n    model_path = f'/kaggle/input/unetplusplus-resnet34/model_fold_{fold + 1}.pth'\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.to(device)\n    model.eval()  # Set model to evaluation mode\n    models.append(model)\n\nprint(f'Loaded {len(models)} models for ensembling.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T18:25:46.807143Z","iopub.execute_input":"2024-11-24T18:25:46.807983Z","iopub.status.idle":"2024-11-24T18:25:53.600755Z","shell.execute_reply.started":"2024-11-24T18:25:46.807948Z","shell.execute_reply":"2024-11-24T18:25:53.599912Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/1655216050.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Loaded 8 models for ensembling.\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"len(models)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T18:25:53.602010Z","iopub.execute_input":"2024-11-24T18:25:53.602292Z","iopub.status.idle":"2024-11-24T18:25:53.607949Z","shell.execute_reply.started":"2024-11-24T18:25:53.602266Z","shell.execute_reply":"2024-11-24T18:25:53.606939Z"}},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"8"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"color_dict= {0: (0, 0, 0),\n             1: (255, 0, 0),\n             2: (0, 255, 0)}\ndef mask_to_rgb(mask, color_dict):\n    output = np.zeros((mask.shape[0], mask.shape[1], 3))\n\n    for k in color_dict.keys():\n        output[mask==k] = color_dict[k]\n\n    return np.uint8(output)   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T18:20:11.170986Z","iopub.execute_input":"2024-11-24T18:20:11.171724Z","iopub.status.idle":"2024-11-24T18:20:11.176638Z","shell.execute_reply.started":"2024-11-24T18:20:11.171689Z","shell.execute_reply":"2024-11-24T18:20:11.175699Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\ndef Smoothed_img(image):\n    resized_image = image\n    smoothed_image = cv2.GaussianBlur(resized_image, (15, 15), 0)\n    red_pixels = (smoothed_image[:, :, 2] > 100)\n    green_pixels = (smoothed_image[:, :, 1] > 100)\n    black_pixels = ~(red_pixels | green_pixels)\n    smoothed_image[red_pixels] = [0, 0, 255]\n    smoothed_image[green_pixels] = [0, 255, 0]\n    smoothed_image[black_pixels] = [0, 0, 0]\n\n    pixel_values = np.array(smoothed_image)\n    # unique_values = np.unique(pixel_values)\n    # print(unique_values)\n    return smoothed_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T18:16:11.552708Z","iopub.execute_input":"2024-11-24T18:16:11.553390Z","iopub.status.idle":"2024-11-24T18:16:11.558975Z","shell.execute_reply.started":"2024-11-24T18:16:11.553355Z","shell.execute_reply":"2024-11-24T18:16:11.558031Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"!mkdir prediction","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:49:35.674166Z","iopub.execute_input":"2024-11-24T17:49:35.674585Z","iopub.status.idle":"2024-11-24T17:49:36.754142Z","shell.execute_reply.started":"2024-11-24T17:49:35.674554Z","shell.execute_reply":"2024-11-24T17:49:36.753097Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"with torch.no_grad():\n    for image, h, w, name in test_loader:\n        outputs_list = []\n        for model in models:\n            outputs = model(image.to(device))\n            outputs = outputs.squeeze().cpu().numpy()\n            outputs_list.append(outputs)\n        \n        # Average the class predictions\n        avg_outputs = np.mean(outputs_list, axis=0)\n        avg_outputs = np.argmax(avg_outputs, axis=0)\n        avg_outputs = mask_to_rgb(avg_outputs, color_dict)\n        \n        w = w.item()\n        h = h.item()\n        new_size = (w, h)\n        resized_image = cv2.resize(avg_outputs, new_size, interpolation=cv2.INTER_CUBIC)\n        resized_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n        resized_image = Smoothed_img(resized_image)\n        cv2.imwrite(f\"prediction/{name[0]}.png\", resized_image)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T18:26:00.019170Z","iopub.execute_input":"2024-11-24T18:26:00.019518Z","iopub.status.idle":"2024-11-24T18:26:47.666550Z","shell.execute_reply.started":"2024-11-24T18:26:00.019489Z","shell.execute_reply":"2024-11-24T18:26:47.665551Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"def rle_to_string(runs):\n    return ' '.join(str(x) for x in runs)\n\ndef rle_encode_one_mask(mask):\n    pixels = mask.flatten()\n    pixels[pixels > 225] = 255\n    pixels[pixels <= 225] = 0\n    use_padding = False\n    if pixels[0] or pixels[-1]:\n        use_padding = True\n        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n        pixel_padded[1:-1] = pixels\n        pixels = pixel_padded\n    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    if use_padding:\n        rle = rle - 1\n    rle[1::2] = rle[1::2] - rle[:-1:2]\n    \n    return rle_to_string(rle)\n\ndef rle2mask(mask_rle, shape=(3,3)):\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T\n\ndef mask2string(dir):\n    strings = []\n    ids = []\n    ws, hs = [[] for i in range(2)]\n    for image_id in os.listdir(dir):\n        id = image_id.split('.')[0]\n        path = os.path.join(dir, image_id)\n        # print(path)\n        img = cv2.imread(path)[:,:,::-1]\n        h, w = img.shape[0], img.shape[1]\n        for channel in range(2):\n            ws.append(w)\n            hs.append(h)\n            ids.append(f'{id}_{channel}')\n            string = rle_encode_one_mask(img[:,:,channel])\n            strings.append(string)\n    r = {\n        'ids': ids,\n        'strings': strings,\n    }\n    return r\n\n\nMASK_DIR_PATH = '/kaggle/working/prediction'\ndir = MASK_DIR_PATH\nres = mask2string(dir)\ndf = pd.DataFrame(columns=['Id', 'Expected'])\ndf['Id'] = res['ids']\ndf['Expected'] = res['strings']\n\ndf.to_csv(r'output.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T18:26:47.667980Z","iopub.execute_input":"2024-11-24T18:26:47.668361Z","iopub.status.idle":"2024-11-24T18:26:50.738191Z","shell.execute_reply.started":"2024-11-24T18:26:47.668325Z","shell.execute_reply":"2024-11-24T18:26:50.737466Z"}},"outputs":[],"execution_count":57}]}