{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81fbcca0",
   "metadata": {
    "papermill": {
     "duration": 0.006516,
     "end_time": "2024-11-21T09:59:50.557062",
     "exception": false,
     "start_time": "2024-11-21T09:59:50.550546",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2f0f094",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T09:59:50.570980Z",
     "iopub.status.busy": "2024-11-21T09:59:50.570683Z",
     "iopub.status.idle": "2024-11-21T10:00:23.156413Z",
     "shell.execute_reply": "2024-11-21T10:00:23.155494Z"
    },
    "papermill": {
     "duration": 32.59552,
     "end_time": "2024-11-21T10:00:23.158680",
     "exception": false,
     "start_time": "2024-11-21T09:59:50.563160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\r\n",
      "  Obtaining dependency information for torchsummary from https://files.pythonhosted.org/packages/7d/18/1474d06f721b86e6a9b9d7392ad68bed711a02f3b61ac43f13c719db50a6/torchsummary-1.5.1-py3-none-any.whl.metadata\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\r\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n",
      "Collecting torchgeometry\r\n",
      "  Obtaining dependency information for torchgeometry from https://files.pythonhosted.org/packages/a6/d6/3f6820c0589bc3876080c59b58a3bad11af746a7b46f364b1cde7972bd72/torchgeometry-0.1.2-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading torchgeometry-0.1.2-py2.py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from torchgeometry) (2.0.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (4.5.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->torchgeometry) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->torchgeometry) (1.3.0)\r\n",
      "Downloading torchgeometry-0.1.2-py2.py3-none-any.whl (42 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: torchgeometry\r\n",
      "Successfully installed torchgeometry-0.1.2\r\n",
      "Collecting segmentation-models-pytorch\r\n",
      "  Obtaining dependency information for segmentation-models-pytorch from https://files.pythonhosted.org/packages/54/dd/0a3417eebc791d1f60ba9949a2c6fcb406ba671c67042179fb270409b17b/segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata\r\n",
      "  Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata (30 kB)\r\n",
      "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\r\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting huggingface-hub>=0.24.6 (from segmentation-models-pytorch)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.24.6 from https://files.pythonhosted.org/packages/60/bf/cea0b9720c32fa01b0c4ec4b16b9f4ae34ca106b202ebbae9f03ab98cd8f/huggingface_hub-0.26.2-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (10.1.0)\r\n",
      "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\r\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (1.16.0)\r\n",
      "Collecting timm==0.9.7 (from segmentation-models-pytorch)\r\n",
      "  Obtaining dependency information for timm==0.9.7 from https://files.pythonhosted.org/packages/7a/bd/2c56be7a3b5bc71cf85a405246b89d5359f942c9f7fb6db6306d9d056092/timm-0.9.7-py3-none-any.whl.metadata\r\n",
      "  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.15.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (4.66.1)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.0.0)\r\n",
      "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\r\n",
      "  Obtaining dependency information for munch from https://files.pythonhosted.org/packages/56/b3/7c69b37f03260a061883bec0e7b05be7117c1b1c85f5212c72c8c2bc3c8c/munch-4.0.0-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.7->segmentation-models-pytorch) (6.0.1)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.7->segmentation-models-pytorch) (0.4.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.12.2)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2023.10.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (21.3)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.31.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (4.5.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.24.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.0.9)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2023.7.22)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\r\n",
      "Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl (109 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\r\n",
      "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\r\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=7b286d43ce8a705df925cb05ffa9691df6f460608ed06ca95de7935a140f0e68\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\r\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60943 sha256=155c1610da31a412c948e499da2815a60396b3af86c812dbcbbfdd9b6ceaa935\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\r\n",
      "Successfully built efficientnet-pytorch pretrainedmodels\r\n",
      "Installing collected packages: munch, huggingface-hub, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\r\n",
      "  Attempting uninstall: huggingface-hub\r\n",
      "    Found existing installation: huggingface-hub 0.17.3\r\n",
      "    Uninstalling huggingface-hub-0.17.3:\r\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\r\n",
      "  Attempting uninstall: timm\r\n",
      "    Found existing installation: timm 0.9.10\r\n",
      "    Uninstalling timm-0.9.10:\r\n",
      "      Successfully uninstalled timm-0.9.10\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.26.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed efficientnet-pytorch-0.7.1 huggingface-hub-0.26.2 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.4 timm-0.9.7\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary\n",
    "!pip install torchgeometry\n",
    "!pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06601bc4",
   "metadata": {
    "papermill": {
     "duration": 0.007853,
     "end_time": "2024-11-21T10:00:23.175244",
     "exception": false,
     "start_time": "2024-11-21T10:00:23.167391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f171463c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:23.193489Z",
     "iopub.status.busy": "2024-11-21T10:00:23.193173Z",
     "iopub.status.idle": "2024-11-21T10:00:31.883834Z",
     "shell.execute_reply": "2024-11-21T10:00:31.883172Z"
    },
    "papermill": {
     "duration": 8.70268,
     "end_time": "2024-11-21T10:00:31.885879",
     "exception": false,
     "start_time": "2024-11-21T10:00:23.183199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "import timm\n",
    "import segmentation_models_pytorch as smp\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0f6b78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:31.904354Z",
     "iopub.status.busy": "2024-11-21T10:00:31.904106Z",
     "iopub.status.idle": "2024-11-21T10:00:32.950192Z",
     "shell.execute_reply": "2024-11-21T10:00:32.949202Z"
    },
    "papermill": {
     "duration": 1.057689,
     "end_time": "2024-11-21T10:00:32.952206",
     "exception": false,
     "start_time": "2024-11-21T10:00:31.894517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-bcfa836f-fa3d-7411-22c5-c5f2f84bc080)\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a75c399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:32.971355Z",
     "iopub.status.busy": "2024-11-21T10:00:32.970985Z",
     "iopub.status.idle": "2024-11-21T10:00:33.046516Z",
     "shell.execute_reply": "2024-11-21T10:00:33.045574Z"
    },
    "papermill": {
     "duration": 0.087149,
     "end_time": "2024-11-21T10:00:33.048400",
     "exception": false,
     "start_time": "2024-11-21T10:00:32.961251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda122d6",
   "metadata": {
    "papermill": {
     "duration": 0.008116,
     "end_time": "2024-11-21T10:00:33.065064",
     "exception": false,
     "start_time": "2024-11-21T10:00:33.056948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c4ee4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:33.082835Z",
     "iopub.status.busy": "2024-11-21T10:00:33.082229Z",
     "iopub.status.idle": "2024-11-21T10:00:34.026121Z",
     "shell.execute_reply": "2024-11-21T10:00:34.025253Z"
    },
    "papermill": {
     "duration": 0.954683,
     "end_time": "2024-11-21T10:00:34.027857",
     "exception": false,
     "start_time": "2024-11-21T10:00:33.073174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_path = \"/kaggle/input/bkai-igh-neopolyp/train/train/\"\n",
    "image_path = []\n",
    "TRAIN_DIR = '/kaggle/input/bkai-igh-neopolyp/train/train'\n",
    "for root, dirs, files in os.walk(TRAIN_DIR):\n",
    "    for file in files:\n",
    "        path = os.path.join(root,file)\n",
    "        image_path.append(path)\n",
    "        \n",
    "len(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "146e8afa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:34.045938Z",
     "iopub.status.busy": "2024-11-21T10:00:34.045665Z",
     "iopub.status.idle": "2024-11-21T10:00:34.049784Z",
     "shell.execute_reply": "2024-11-21T10:00:34.048960Z"
    },
    "papermill": {
     "duration": 0.015045,
     "end_time": "2024-11-21T10:00:34.051431",
     "exception": false,
     "start_time": "2024-11-21T10:00:34.036386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# def calculate_mean_image_size(image_dir):\n",
    "#     widths = []\n",
    "#     heights = []\n",
    "\n",
    "#     for filename in os.listdir(image_dir):\n",
    "#         if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "#             img_path = os.path.join(image_dir, filename)\n",
    "#             image = cv2.imread(img_path)\n",
    "#             if image is not None:\n",
    "#                 h, w = image.shape[:2]\n",
    "#                 heights.append(h)\n",
    "#                 widths.append(w)\n",
    "#             else:\n",
    "#                 print(f\"Warning: Unable to read image {img_path}\")\n",
    "\n",
    "#     mean_width = int(np.mean(widths))\n",
    "#     mean_height = int(np.mean(heights))\n",
    "#     median_width = int(np.median(widths))\n",
    "#     median_height = int(np.median(heights))\n",
    "\n",
    "#     print(f\"Number of images: {len(widths)}\")\n",
    "#     print(f\"Mean Width: {mean_width}, Mean Height: {mean_height}\")\n",
    "#     print(f\"Median Width: {median_width}, Median Height: {median_height}\")\n",
    "\n",
    "#     return mean_width, mean_height, widths, heights\n",
    "\n",
    "# # Example usage:\n",
    "# image_directory = '/kaggle/input/bkai-igh-neopolyp/test/test'\n",
    "# mean_w, mean_h, widths, heights = calculate_mean_image_size(image_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a057a719",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:34.068767Z",
     "iopub.status.busy": "2024-11-21T10:00:34.068528Z",
     "iopub.status.idle": "2024-11-21T10:00:34.072343Z",
     "shell.execute_reply": "2024-11-21T10:00:34.071561Z"
    },
    "papermill": {
     "duration": 0.014316,
     "end_time": "2024-11-21T10:00:34.073922",
     "exception": false,
     "start_time": "2024-11-21T10:00:34.059606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_dimension_distribution(widths, heights):\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.hist(widths, bins=20, color='skyblue')\n",
    "#     plt.title('Width Distribution')\n",
    "#     plt.xlabel('Width')\n",
    "#     plt.ylabel('Frequency')\n",
    "\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.hist(heights, bins=20, color='salmon')\n",
    "#     plt.title('Height Distribution')\n",
    "#     plt.xlabel('Height')\n",
    "#     plt.ylabel('Frequency')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # After collecting widths and heights in the function above:\n",
    "# plot_dimension_distribution(widths, heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c036d1a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:34.131143Z",
     "iopub.status.busy": "2024-11-21T10:00:34.130490Z",
     "iopub.status.idle": "2024-11-21T10:00:35.106755Z",
     "shell.execute_reply": "2024-11-21T10:00:35.105809Z"
    },
    "papermill": {
     "duration": 0.987385,
     "end_time": "2024-11-21T10:00:35.108589",
     "exception": false,
     "start_time": "2024-11-21T10:00:34.121204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_path = []\n",
    "TRAIN_MASK_DIR = '/kaggle/input/bkai-igh-neopolyp/train_gt/train_gt'\n",
    "for root, dirs, files in os.walk(TRAIN_MASK_DIR):\n",
    "    for file in files:\n",
    "        path = os.path.join(root,file)\n",
    "        mask_path.append(path)\n",
    "        \n",
    "len(mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a24b0bec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:35.127327Z",
     "iopub.status.busy": "2024-11-21T10:00:35.126999Z",
     "iopub.status.idle": "2024-11-21T10:00:35.135779Z",
     "shell.execute_reply": "2024-11-21T10:00:35.134961Z"
    },
    "papermill": {
     "duration": 0.020047,
     "end_time": "2024-11-21T10:00:35.137467",
     "exception": false,
     "start_time": "2024-11-21T10:00:35.117420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DatasetCustom(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, resize=None, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.resize = resize\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(self.img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def read_mask(self, mask_path):\n",
    "        image = cv2.imread(mask_path)\n",
    "        image = cv2.resize(image, self.resize, interpolation=cv2.INTER_AREA)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        lower_red1 = np.array([0, 100, 20])\n",
    "        upper_red1 = np.array([10, 255, 255])\n",
    "        lower_red2 = np.array([160,100,20])\n",
    "        upper_red2 = np.array([179,255,255])\n",
    "        \n",
    "        lower_mask_red = cv2.inRange(image, lower_red1, upper_red1)\n",
    "        upper_mask_red = cv2.inRange(image, lower_red2, upper_red2)\n",
    "        \n",
    "        red_mask = lower_mask_red + upper_mask_red\n",
    "        red_mask[red_mask != 0] = 1\n",
    "\n",
    "        green_mask = cv2.inRange(image, (36, 25, 25), (70, 255, 255))\n",
    "        green_mask[green_mask != 0] = 2\n",
    "\n",
    "        full_mask = cv2.bitwise_or(red_mask, green_mask)\n",
    "        full_mask = np.expand_dims(full_mask, axis=-1) \n",
    "        full_mask = full_mask.astype(np.uint8)\n",
    "        \n",
    "        return full_mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.images[idx])\n",
    "        image = cv2.imread(img_path)  #  BGR\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Convert to RGB\n",
    "        label = self.read_mask(label_path)  \n",
    "        image = cv2.resize(image, self.resize, interpolation=cv2.INTER_AREA)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da43dc63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:35.155358Z",
     "iopub.status.busy": "2024-11-21T10:00:35.154845Z",
     "iopub.status.idle": "2024-11-21T10:00:35.158449Z",
     "shell.execute_reply": "2024-11-21T10:00:35.157673Z"
    },
    "papermill": {
     "duration": 0.014262,
     "end_time": "2024-11-21T10:00:35.160000",
     "exception": false,
     "start_time": "2024-11-21T10:00:35.145738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_resize = (256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0eb2ba52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:35.177606Z",
     "iopub.status.busy": "2024-11-21T10:00:35.177361Z",
     "iopub.status.idle": "2024-11-21T10:00:35.181260Z",
     "shell.execute_reply": "2024-11-21T10:00:35.180650Z"
    },
    "papermill": {
     "duration": 0.014512,
     "end_time": "2024-11-21T10:00:35.182843",
     "exception": false,
     "start_time": "2024-11-21T10:00:35.168331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = DatasetCustom(img_dir= TRAIN_DIR,\n",
    "                             label_dir= TRAIN_MASK_DIR,\n",
    "                             resize= img_resize,\n",
    "                             transform = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017667b5",
   "metadata": {
    "papermill": {
     "duration": 0.008463,
     "end_time": "2024-11-21T10:00:35.199795",
     "exception": false,
     "start_time": "2024-11-21T10:00:35.191332",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b31b240",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:35.218374Z",
     "iopub.status.busy": "2024-11-21T10:00:35.217670Z",
     "iopub.status.idle": "2024-11-21T10:00:35.223308Z",
     "shell.execute_reply": "2024-11-21T10:00:35.222552Z"
    },
    "papermill": {
     "duration": 0.016562,
     "end_time": "2024-11-21T10:00:35.224960",
     "exception": false,
     "start_time": "2024-11-21T10:00:35.208398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AugmentDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None, length_multiplier=1):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.length_multiplier = length_multiplier\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = idx % len(self.dataset)\n",
    "        image, label = self.dataset[actual_idx]\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=label)\n",
    "            image = transformed['image'].float()\n",
    "            label = transformed['mask'].float()\n",
    "            label = label.permute(2, 0, 1)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) * self.length_multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8412433",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:35.242673Z",
     "iopub.status.busy": "2024-11-21T10:00:35.242423Z",
     "iopub.status.idle": "2024-11-21T10:00:35.248426Z",
     "shell.execute_reply": "2024-11-21T10:00:35.247772Z"
    },
    "papermill": {
     "duration": 0.01669,
     "end_time": "2024-11-21T10:00:35.249973",
     "exception": false,
     "start_time": "2024-11-21T10:00:35.233283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transformation = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomGamma (gamma_limit=(70, 130), eps=None, always_apply=False, p=0.2),\n",
    "    A.RGBShift(p=0.3, r_shift_limit=10, g_shift_limit=10, b_shift_limit=10),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transformation = A.Compose([\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f2cd68e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:35.267816Z",
     "iopub.status.busy": "2024-11-21T10:00:35.267574Z",
     "iopub.status.idle": "2024-11-21T10:00:35.280317Z",
     "shell.execute_reply": "2024-11-21T10:00:35.279525Z"
    },
    "papermill": {
     "duration": 0.023582,
     "end_time": "2024-11-21T10:00:35.282082",
     "exception": false,
     "start_time": "2024-11-21T10:00:35.258500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the split sizes\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "# Randomly split the dataset\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "train_dataset = AugmentDataset(train_dataset, transform=train_transformation, length_multiplier=1)\n",
    "val_dataset = AugmentDataset(val_dataset, transform=val_transformation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e60ff3a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:35.300448Z",
     "iopub.status.busy": "2024-11-21T10:00:35.300170Z",
     "iopub.status.idle": "2024-11-21T10:00:35.305298Z",
     "shell.execute_reply": "2024-11-21T10:00:35.304571Z"
    },
    "papermill": {
     "duration": 0.016171,
     "end_time": "2024-11-21T10:00:35.306908",
     "exception": false,
     "start_time": "2024-11-21T10:00:35.290737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb83db",
   "metadata": {
    "papermill": {
     "duration": 0.00813,
     "end_time": "2024-11-21T10:00:35.323525",
     "exception": false,
     "start_time": "2024-11-21T10:00:35.315395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b141a97b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:35.341038Z",
     "iopub.status.busy": "2024-11-21T10:00:35.340770Z",
     "iopub.status.idle": "2024-11-21T10:00:36.829415Z",
     "shell.execute_reply": "2024-11-21T10:00:36.828390Z"
    },
    "papermill": {
     "duration": 1.499802,
     "end_time": "2024-11-21T10:00:36.831546",
     "exception": false,
     "start_time": "2024-11-21T10:00:35.331744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 327MB/s]\n"
     ]
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=\"resnet50\",        \n",
    "    # encoder_weights=\"imagenet\",     \n",
    "    in_channels=3,                  \n",
    "    classes=3     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c693dd9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:36.851417Z",
     "iopub.status.busy": "2024-11-21T10:00:36.850759Z",
     "iopub.status.idle": "2024-11-21T10:00:42.323085Z",
     "shell.execute_reply": "2024-11-21T10:00:42.322235Z"
    },
    "papermill": {
     "duration": 5.483771,
     "end_time": "2024-11-21T10:00:42.324836",
     "exception": false,
     "start_time": "2024-11-21T10:00:36.841065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnetPlusPlus(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): UnetPlusPlusDecoder(\n",
       "    (center): Identity()\n",
       "    (blocks): ModuleDict(\n",
       "      (x_0_0): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(3072, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_0_1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(1280, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_1_1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(1536, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_0_2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(896, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_1_2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_2_2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_0_3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(320, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_1_3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(448, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_2_3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_3_3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_0_4): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('/kaggle/input/kaggle-notebook/model.pth')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3eed92f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:42.345178Z",
     "iopub.status.busy": "2024-11-21T10:00:42.344897Z",
     "iopub.status.idle": "2024-11-21T10:00:42.349586Z",
     "shell.execute_reply": "2024-11-21T10:00:42.348793Z"
    },
    "papermill": {
     "duration": 0.016415,
     "end_time": "2024-11-21T10:00:42.351304",
     "exception": false,
     "start_time": "2024-11-21T10:00:42.334889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "color_dict= {0: (0, 0, 0),\n",
    "             1: (255, 0, 0),\n",
    "             2: (0, 255, 0)}\n",
    "def mask_to_rgb(mask, color_dict):\n",
    "    output = np.zeros((mask.shape[0], mask.shape[1], 3))\n",
    "\n",
    "    for k in color_dict.keys():\n",
    "        output[mask==k] = color_dict[k]\n",
    "\n",
    "    return np.uint8(output)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb272995",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:42.370334Z",
     "iopub.status.busy": "2024-11-21T10:00:42.370096Z",
     "iopub.status.idle": "2024-11-21T10:00:42.373464Z",
     "shell.execute_reply": "2024-11-21T10:00:42.372869Z"
    },
    "papermill": {
     "duration": 0.014675,
     "end_time": "2024-11-21T10:00:42.374997",
     "exception": false,
     "start_time": "2024-11-21T10:00:42.360322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del dataset\n",
    "del train_dataset\n",
    "del val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59af3e0e",
   "metadata": {
    "papermill": {
     "duration": 0.008944,
     "end_time": "2024-11-21T10:00:42.393250",
     "exception": false,
     "start_time": "2024-11-21T10:00:42.384306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64d9056",
   "metadata": {
    "papermill": {
     "duration": 0.008975,
     "end_time": "2024-11-21T10:00:42.411194",
     "exception": false,
     "start_time": "2024-11-21T10:00:42.402219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Define Dice loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82c7a2c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:42.430624Z",
     "iopub.status.busy": "2024-11-21T10:00:42.430397Z",
     "iopub.status.idle": "2024-11-21T10:00:42.436277Z",
     "shell.execute_reply": "2024-11-21T10:00:42.435547Z"
    },
    "papermill": {
     "duration": 0.017287,
     "end_time": "2024-11-21T10:00:42.437883",
     "exception": false,
     "start_time": "2024-11-21T10:00:42.420596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, num_classes=3, epsilon=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        outputs = F.softmax(outputs, dim=1)\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=self.num_classes).permute(0, 3, 1, 2).float()\n",
    "\n",
    "        # Flatten the tensors\n",
    "        outputs_flat = outputs.contiguous().view(-1, self.num_classes)\n",
    "        targets_flat = targets_one_hot.contiguous().view(-1, self.num_classes)\n",
    "\n",
    "        intersection = torch.sum(outputs_flat * targets_flat, dim=0)\n",
    "        union = torch.sum(outputs_flat + targets_flat, dim=0)\n",
    "\n",
    "        dice_loss = 1 - (2 * intersection + self.epsilon) / (union + self.epsilon)\n",
    "        mean_dice_loss = dice_loss.mean()\n",
    "\n",
    "        return mean_dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acbdf02b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:42.456967Z",
     "iopub.status.busy": "2024-11-21T10:00:42.456759Z",
     "iopub.status.idle": "2024-11-21T10:00:42.460621Z",
     "shell.execute_reply": "2024-11-21T10:00:42.459979Z"
    },
    "papermill": {
     "duration": 0.015224,
     "end_time": "2024-11-21T10:00:42.462098",
     "exception": false,
     "start_time": "2024-11-21T10:00:42.446874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# model.to(device)\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "criterion_dice = DiceLoss(num_classes=3)\n",
    "alpha = 0.5  # Weight for balancing the two losses\n",
    "best_val_loss = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb40f855",
   "metadata": {
    "papermill": {
     "duration": 0.009079,
     "end_time": "2024-11-21T10:00:42.480364",
     "exception": false,
     "start_time": "2024-11-21T10:00:42.471285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Make use of learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9f20ff6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:42.500081Z",
     "iopub.status.busy": "2024-11-21T10:00:42.499620Z",
     "iopub.status.idle": "2024-11-21T10:00:42.505684Z",
     "shell.execute_reply": "2024-11-21T10:00:42.504796Z"
    },
    "papermill": {
     "duration": 0.017611,
     "end_time": "2024-11-21T10:00:42.507280",
     "exception": false,
     "start_time": "2024-11-21T10:00:42.489669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Assuming you've already defined your optimizer\n",
    "learning_rate = 0.00005\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize the scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min',                 # We want to minimize the validation loss\n",
    "    factor=0.5,                 # Reduce LR by a factor of 0.5\n",
    "    patience=1,                 # Number of epochs with no improvement after which LR will be reduced\n",
    "    verbose=True,               # Print a message when LR is updated\n",
    "    threshold=0.0001,           # Threshold for measuring the new optimum\n",
    "    threshold_mode='rel',       # Mode for threshold\n",
    "    cooldown=0,                 # Number of epochs to wait before resuming normal operation after LR has been reduced\n",
    "    min_lr=1e-6                 # Lower bound on the learning rate\n",
    ")\n",
    "\n",
    "# scheduler2 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "875b57b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:00:42.527661Z",
     "iopub.status.busy": "2024-11-21T10:00:42.526851Z",
     "iopub.status.idle": "2024-11-21T10:01:12.601934Z",
     "shell.execute_reply": "2024-11-21T10:01:12.600991Z"
    },
    "papermill": {
     "duration": 30.087225,
     "end_time": "2024-11-21T10:01:12.603790",
     "exception": false,
     "start_time": "2024-11-21T10:00:42.516565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtu7pham7\u001b[0m (\u001b[33mhustcollab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20241121_100043-a6hcyhbg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msnowy-lake-32\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/hustcollab/PolypSegment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/hustcollab/PolypSegment/runs/a6hcyhbg\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(\n",
    "    # set the wandb project where this run will be logged\n",
    "#     project= \"PolypSegment\", \n",
    "    key = \"e02f7703b40a2b3e0ab4801d4cb1d86b3b7327a6\",\n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "    project=\"PolypSegment\",\n",
    "    config={\n",
    "        \"init_learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"loss_functions\": [\"CrossEntropy\", \"Dice\"],\n",
    "        \"alpha\": alpha\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add the model to WandB\n",
    "wandb.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e49ada17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:01:12.625758Z",
     "iopub.status.busy": "2024-11-21T10:01:12.625485Z",
     "iopub.status.idle": "2024-11-21T10:34:06.706133Z",
     "shell.execute_reply": "2024-11-21T10:34:06.705331Z"
    },
    "papermill": {
     "duration": 1974.109343,
     "end_time": "2024-11-21T10:34:06.723439",
     "exception": false,
     "start_time": "2024-11-21T10:01:12.614096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.0311518977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:   3%|▎         | 1/30 [01:16<36:56, 76.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/30], Loss: 0.0309893665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:   7%|▋         | 2/30 [02:23<32:59, 70.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/30], Loss: 0.0263229473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  10%|█         | 3/30 [03:29<31:00, 68.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/30], Loss: 0.0248341395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  13%|█▎        | 4/30 [04:35<29:21, 67.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30], Loss: 0.0241136306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  17%|█▋        | 5/30 [05:42<28:02, 67.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/30], Loss: 0.0240635084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  20%|██        | 6/30 [06:48<26:41, 66.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/30], Loss: 0.0225356796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  23%|██▎       | 7/30 [07:53<25:27, 66.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/30], Loss: 0.0212436039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  27%|██▋       | 8/30 [08:59<24:16, 66.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/30], Loss: 0.0207161539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  30%|███       | 9/30 [10:05<23:08, 66.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], Loss: 0.0199139324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  37%|███▋      | 11/30 [12:16<20:48, 65.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/30], Loss: 0.0234532932\n",
      "Epoch [12/30], Loss: 0.0195276257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  40%|████      | 12/30 [13:22<19:42, 65.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/30], Loss: 0.0192166347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  43%|████▎     | 13/30 [14:28<18:37, 65.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/30], Loss: 0.0186824125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  47%|████▋     | 14/30 [15:33<17:31, 65.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/30], Loss: 0.0185604298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  50%|█████     | 15/30 [16:39<16:25, 65.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/30], Loss: 0.0172240197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  57%|█████▋    | 17/30 [18:49<14:07, 65.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/30], Loss: 0.0178183293\n",
      "Epoch [18/30], Loss: 0.0165840590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  63%|██████▎   | 19/30 [20:59<11:55, 65.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/30], Loss: 0.0169014283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  67%|██████▋   | 20/30 [22:03<10:48, 64.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30], Loss: 0.0169245832\n",
      "Epoch 00020: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch [21/30], Loss: 0.0159121246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  73%|███████▎  | 22/30 [24:13<08:38, 64.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/30], Loss: 0.0165278463\n",
      "Epoch [23/30], Loss: 0.0152654396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  80%|████████  | 24/30 [26:23<06:29, 64.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/30], Loss: 0.0155612492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  83%|████████▎ | 25/30 [27:28<05:23, 64.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/30], Loss: 0.0158291118\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.2500e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  87%|████████▋ | 26/30 [28:33<04:19, 64.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/30], Loss: 0.0163154948\n",
      "Epoch [27/30], Loss: 0.0151190166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  93%|█████████▎| 28/30 [30:44<02:10, 65.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/30], Loss: 0.0154281719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  97%|█████████▋| 29/30 [31:49<01:05, 65.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/30], Loss: 0.0154385114\n",
      "Epoch 00029: reducing learning rate of group 0 to 6.2500e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress: 100%|██████████| 30/30 [32:54<00:00, 65.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/30], Loss: 0.0158866987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "epoch_bar = tqdm(total=num_epochs, desc='Total Progress')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        labels = labels.squeeze(dim=1).long()\n",
    "        outputs = model(images)\n",
    "    \n",
    "        loss_ce = criterion_ce(outputs, labels)\n",
    "        loss_dice = criterion_dice(outputs, labels)\n",
    "        loss = alpha * loss_ce + (1 - alpha) * loss_dice\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "#         del images, labels, outputs, loss\n",
    "#         torch.cuda.empty_cache()\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels = labels.squeeze(dim=1).long()\n",
    "            \n",
    "            outputs = model(images)\n",
    "\n",
    "            val_loss += criterion_dice(outputs.float(),labels.long()).item()\n",
    "            \n",
    "#             del images, labels, outputs\n",
    "#             torch.cuda.empty_cache()\n",
    "            \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {val_loss/len(val_loader):.10f}\")\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        checkpoint = { \n",
    "            'epoch': epoch,\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'loss': val_loss,\n",
    "        }\n",
    "        save_path = f'model.pth'\n",
    "        torch.save(checkpoint, save_path)\n",
    "        \n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    epoch_bar.update(1)\n",
    "    wandb.log({'Val_loss': val_loss/len(val_loader),'Train_loss': train_loss/len(train_loader), 'Learning_rate' : current_lr})\n",
    "epoch_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e5f3965",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:34:06.754087Z",
     "iopub.status.busy": "2024-11-21T10:34:06.753340Z",
     "iopub.status.idle": "2024-11-21T10:34:07.201382Z",
     "shell.execute_reply": "2024-11-21T10:34:07.200473Z"
    },
    "papermill": {
     "duration": 0.466263,
     "end_time": "2024-11-21T10:34:07.203375",
     "exception": false,
     "start_time": "2024-11-21T10:34:06.737112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnetPlusPlus(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): UnetPlusPlusDecoder(\n",
       "    (center): Identity()\n",
       "    (blocks): ModuleDict(\n",
       "      (x_0_0): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(3072, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_0_1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(1280, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_1_1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(1536, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_0_2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(896, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_1_2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_2_2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_0_3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(320, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_1_3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(448, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_2_3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_3_3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_0_4): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = smp.UnetPlusPlus(\n",
    "#     encoder_name=\"resnet34\",        \n",
    "#     encoder_weights=\"imagenet\",     \n",
    "#     in_channels=3,                  \n",
    "#     classes=3     \n",
    "# )\n",
    "checkpoint = torch.load('/kaggle/working/model.pth')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96fdaf15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:34:07.231165Z",
     "iopub.status.busy": "2024-11-21T10:34:07.230883Z",
     "iopub.status.idle": "2024-11-21T10:34:08.263479Z",
     "shell.execute_reply": "2024-11-21T10:34:08.262263Z"
    },
    "papermill": {
     "duration": 1.048422,
     "end_time": "2024-11-21T10:34:08.265494",
     "exception": false,
     "start_time": "2024-11-21T10:34:07.217072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035d563f",
   "metadata": {
    "papermill": {
     "duration": 0.013497,
     "end_time": "2024-11-21T10:34:08.292651",
     "exception": false,
     "start_time": "2024-11-21T10:34:08.279154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Get Testset Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddfc9389",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:34:08.321771Z",
     "iopub.status.busy": "2024-11-21T10:34:08.321003Z",
     "iopub.status.idle": "2024-11-21T10:34:34.711183Z",
     "shell.execute_reply": "2024-11-21T10:34:34.710455Z"
    },
    "papermill": {
     "duration": 26.407082,
     "end_time": "2024-11-21T10:34:34.713458",
     "exception": false,
     "start_time": "2024-11-21T10:34:08.306376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for i in os.listdir(\"/kaggle/input/bkai-igh-neopolyp/test/test\"):\n",
    "    img_path = os.path.join(\"/kaggle/input/bkai-igh-neopolyp/test/test\", i)\n",
    "    ori_img = cv2.imread(img_path)\n",
    "    ori_img = cv2.cvtColor(ori_img, cv2.COLOR_BGR2RGB)\n",
    "    ori_h = ori_img.shape[0]\n",
    "    ori_w = ori_img.shape[1]\n",
    "    img = cv2.resize(ori_img, img_resize, interpolation=cv2.INTER_AREA)\n",
    "    transformed = val_transformation(image=img)\n",
    "    input_img = transformed[\"image\"]\n",
    "    input_img = input_img.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output_mask = model(input_img).squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "    mask = cv2.resize(output_mask, (ori_w, ori_h), interpolation=cv2.INTER_CUBIC)\n",
    "    mask = np.argmax(mask, axis=2)\n",
    "    mask_rgb = mask_to_rgb(mask, color_dict)\n",
    "    mask_rgb = cv2.cvtColor(mask_rgb, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(\"prediction/{}\".format(i), mask_rgb)\n",
    "    # Clear variables to free memory\n",
    "    del img, input_img, output_mask, mask, mask_rgb\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9531f3a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:34:34.741825Z",
     "iopub.status.busy": "2024-11-21T10:34:34.741506Z",
     "iopub.status.idle": "2024-11-21T10:34:37.534165Z",
     "shell.execute_reply": "2024-11-21T10:34:37.533483Z"
    },
    "papermill": {
     "duration": 2.80928,
     "end_time": "2024-11-21T10:34:37.536426",
     "exception": false,
     "start_time": "2024-11-21T10:34:34.727146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rle_to_string(runs):\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_encode_one_mask(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels[pixels > 225] = 255\n",
    "    pixels[pixels <= 225] = 0\n",
    "    use_padding = False\n",
    "    if pixels[0] or pixels[-1]:\n",
    "        use_padding = True\n",
    "        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n",
    "        pixel_padded[1:-1] = pixels\n",
    "        pixels = pixel_padded\n",
    "    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    if use_padding:\n",
    "        rle = rle - 1\n",
    "    rle[1::2] = rle[1::2] - rle[:-1:2]\n",
    "    \n",
    "    return rle_to_string(rle)\n",
    "\n",
    "def rle2mask(mask_rle, shape=(3,3)):\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T\n",
    "\n",
    "def mask2string(dir):\n",
    "    strings = []\n",
    "    ids = []\n",
    "    ws, hs = [[] for i in range(2)]\n",
    "    for image_id in os.listdir(dir):\n",
    "        id = image_id.split('.')[0]\n",
    "        path = os.path.join(dir, image_id)\n",
    "        # print(path)\n",
    "        img = cv2.imread(path)[:,:,::-1]\n",
    "        h, w = img.shape[0], img.shape[1]\n",
    "        for channel in range(2):\n",
    "            ws.append(w)\n",
    "            hs.append(h)\n",
    "            ids.append(f'{id}_{channel}')\n",
    "            string = rle_encode_one_mask(img[:,:,channel])\n",
    "            strings.append(string)\n",
    "    r = {\n",
    "        'ids': ids,\n",
    "        'strings': strings,\n",
    "    }\n",
    "    return r\n",
    "\n",
    "\n",
    "MASK_DIR_PATH = '/kaggle/working/prediction'\n",
    "dir = MASK_DIR_PATH\n",
    "res = mask2string(dir)\n",
    "df = pd.DataFrame(columns=['Id', 'Expected'])\n",
    "df['Id'] = res['ids']\n",
    "df['Expected'] = res['strings']\n",
    "\n",
    "df.to_csv(r'output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bad436",
   "metadata": {
    "papermill": {
     "duration": 0.013106,
     "end_time": "2024-11-21T10:34:37.563238",
     "exception": false,
     "start_time": "2024-11-21T10:34:37.550132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f7f46e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:34:37.590927Z",
     "iopub.status.busy": "2024-11-21T10:34:37.590254Z",
     "iopub.status.idle": "2024-11-21T10:34:37.593964Z",
     "shell.execute_reply": "2024-11-21T10:34:37.593301Z"
    },
    "papermill": {
     "duration": 0.019212,
     "end_time": "2024-11-21T10:34:37.595588",
     "exception": false,
     "start_time": "2024-11-21T10:34:37.576376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torch.jit import load\n",
    "# model = UNet()\n",
    "# optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# checkpoint = torch.load(pretrained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "608ce552",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:34:37.622042Z",
     "iopub.status.busy": "2024-11-21T10:34:37.621805Z",
     "iopub.status.idle": "2024-11-21T10:34:37.625335Z",
     "shell.execute_reply": "2024-11-21T10:34:37.624673Z"
    },
    "papermill": {
     "duration": 0.018479,
     "end_time": "2024-11-21T10:34:37.626882",
     "exception": false,
     "start_time": "2024-11-21T10:34:37.608403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c70c24d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:34:37.653391Z",
     "iopub.status.busy": "2024-11-21T10:34:37.653147Z",
     "iopub.status.idle": "2024-11-21T10:34:37.656574Z",
     "shell.execute_reply": "2024-11-21T10:34:37.655802Z"
    },
    "papermill": {
     "duration": 0.018365,
     "end_time": "2024-11-21T10:34:37.658116",
     "exception": false,
     "start_time": "2024-11-21T10:34:37.639751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from collections import OrderedDict\n",
    "# new_state_dict = OrderedDict()\n",
    "# for k, v in checkpoint['model'].items():\n",
    "#     name = k[7:] # remove `module.`\n",
    "#     new_state_dict[name] = v\n",
    "# # load params\n",
    "# model.load_state_dict(new_state_dict)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 2715462,
     "sourceId": 30892,
     "sourceType": "competition"
    },
    {
     "sourceId": 208782724,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30580,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2092.876943,
   "end_time": "2024-11-21T10:34:40.391544",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-21T09:59:47.514601",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
