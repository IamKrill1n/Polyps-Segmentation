{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee54795a",
   "metadata": {
    "papermill": {
     "duration": 0.006386,
     "end_time": "2024-11-21T17:20:15.302357",
     "exception": false,
     "start_time": "2024-11-21T17:20:15.295971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f70593b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:20:15.315354Z",
     "iopub.status.busy": "2024-11-21T17:20:15.315043Z",
     "iopub.status.idle": "2024-11-21T17:20:47.617249Z",
     "shell.execute_reply": "2024-11-21T17:20:47.616366Z"
    },
    "papermill": {
     "duration": 32.31098,
     "end_time": "2024-11-21T17:20:47.619324",
     "exception": false,
     "start_time": "2024-11-21T17:20:15.308344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\r\n",
      "  Obtaining dependency information for torchsummary from https://files.pythonhosted.org/packages/7d/18/1474d06f721b86e6a9b9d7392ad68bed711a02f3b61ac43f13c719db50a6/torchsummary-1.5.1-py3-none-any.whl.metadata\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\r\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n",
      "Collecting torchgeometry\r\n",
      "  Obtaining dependency information for torchgeometry from https://files.pythonhosted.org/packages/a6/d6/3f6820c0589bc3876080c59b58a3bad11af746a7b46f364b1cde7972bd72/torchgeometry-0.1.2-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading torchgeometry-0.1.2-py2.py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from torchgeometry) (2.0.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (4.5.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->torchgeometry) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->torchgeometry) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->torchgeometry) (1.3.0)\r\n",
      "Downloading torchgeometry-0.1.2-py2.py3-none-any.whl (42 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: torchgeometry\r\n",
      "Successfully installed torchgeometry-0.1.2\r\n",
      "Collecting segmentation-models-pytorch\r\n",
      "  Obtaining dependency information for segmentation-models-pytorch from https://files.pythonhosted.org/packages/54/dd/0a3417eebc791d1f60ba9949a2c6fcb406ba671c67042179fb270409b17b/segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata\r\n",
      "  Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata (30 kB)\r\n",
      "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\r\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting huggingface-hub>=0.24.6 (from segmentation-models-pytorch)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.24.6 from https://files.pythonhosted.org/packages/60/bf/cea0b9720c32fa01b0c4ec4b16b9f4ae34ca106b202ebbae9f03ab98cd8f/huggingface_hub-0.26.2-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (10.1.0)\r\n",
      "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\r\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (1.16.0)\r\n",
      "Collecting timm==0.9.7 (from segmentation-models-pytorch)\r\n",
      "  Obtaining dependency information for timm==0.9.7 from https://files.pythonhosted.org/packages/7a/bd/2c56be7a3b5bc71cf85a405246b89d5359f942c9f7fb6db6306d9d056092/timm-0.9.7-py3-none-any.whl.metadata\r\n",
      "  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (0.15.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from segmentation-models-pytorch) (4.66.1)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.0.0)\r\n",
      "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\r\n",
      "  Obtaining dependency information for munch from https://files.pythonhosted.org/packages/56/b3/7c69b37f03260a061883bec0e7b05be7117c1b1c85f5212c72c8c2bc3c8c/munch-4.0.0-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm==0.9.7->segmentation-models-pytorch) (6.0.1)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm==0.9.7->segmentation-models-pytorch) (0.4.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.12.2)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2023.10.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (21.3)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.31.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (4.5.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.24.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.0.9)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2023.7.22)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\r\n",
      "Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl (109 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\r\n",
      "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\r\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=3c7998def1fed5c3ed132cee17c029f4fa8c7b5245aebe253f36dbefc39a7336\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\r\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60943 sha256=96c19940086483438a380d1d94b0e2fcb4b67441e9cd0d0c7b3df4ba049890f8\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\r\n",
      "Successfully built efficientnet-pytorch pretrainedmodels\r\n",
      "Installing collected packages: munch, huggingface-hub, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\r\n",
      "  Attempting uninstall: huggingface-hub\r\n",
      "    Found existing installation: huggingface-hub 0.17.3\r\n",
      "    Uninstalling huggingface-hub-0.17.3:\r\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\r\n",
      "  Attempting uninstall: timm\r\n",
      "    Found existing installation: timm 0.9.10\r\n",
      "    Uninstalling timm-0.9.10:\r\n",
      "      Successfully uninstalled timm-0.9.10\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.26.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed efficientnet-pytorch-0.7.1 huggingface-hub-0.26.2 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.4 timm-0.9.7\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary\n",
    "!pip install torchgeometry\n",
    "!pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241b2497",
   "metadata": {
    "papermill": {
     "duration": 0.007417,
     "end_time": "2024-11-21T17:20:47.634756",
     "exception": false,
     "start_time": "2024-11-21T17:20:47.627339",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8882c2d9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-21T17:20:47.651977Z",
     "iopub.status.busy": "2024-11-21T17:20:47.651696Z",
     "iopub.status.idle": "2024-11-21T17:20:56.090660Z",
     "shell.execute_reply": "2024-11-21T17:20:56.089993Z"
    },
    "papermill": {
     "duration": 8.450437,
     "end_time": "2024-11-21T17:20:56.092711",
     "exception": false,
     "start_time": "2024-11-21T17:20:47.642274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "import timm\n",
    "import segmentation_models_pytorch as smp\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "967a2a8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:20:56.110232Z",
     "iopub.status.busy": "2024-11-21T17:20:56.109599Z",
     "iopub.status.idle": "2024-11-21T17:20:57.138738Z",
     "shell.execute_reply": "2024-11-21T17:20:57.137887Z"
    },
    "papermill": {
     "duration": 1.039739,
     "end_time": "2024-11-21T17:20:57.140695",
     "exception": false,
     "start_time": "2024-11-21T17:20:56.100956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-db0fbe91-5803-41ac-d3f9-b8dedd820f9f)\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17654b22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:20:57.159509Z",
     "iopub.status.busy": "2024-11-21T17:20:57.159227Z",
     "iopub.status.idle": "2024-11-21T17:20:57.223834Z",
     "shell.execute_reply": "2024-11-21T17:20:57.222944Z"
    },
    "papermill": {
     "duration": 0.075422,
     "end_time": "2024-11-21T17:20:57.225634",
     "exception": false,
     "start_time": "2024-11-21T17:20:57.150212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba0d55c",
   "metadata": {
    "papermill": {
     "duration": 0.007548,
     "end_time": "2024-11-21T17:20:57.241097",
     "exception": false,
     "start_time": "2024-11-21T17:20:57.233549",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c2a43f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:20:57.257443Z",
     "iopub.status.busy": "2024-11-21T17:20:57.257160Z",
     "iopub.status.idle": "2024-11-21T17:20:58.056932Z",
     "shell.execute_reply": "2024-11-21T17:20:58.056101Z"
    },
    "papermill": {
     "duration": 0.809892,
     "end_time": "2024-11-21T17:20:58.058682",
     "exception": false,
     "start_time": "2024-11-21T17:20:57.248790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_path = \"/kaggle/input/bkai-igh-neopolyp/train/train/\"\n",
    "image_path = []\n",
    "TRAIN_DIR = '/kaggle/input/bkai-igh-neopolyp/train/train'\n",
    "for root, dirs, files in os.walk(TRAIN_DIR):\n",
    "    for file in files:\n",
    "        path = os.path.join(root,file)\n",
    "        image_path.append(path)\n",
    "        \n",
    "len(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dad31c90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:20:58.076239Z",
     "iopub.status.busy": "2024-11-21T17:20:58.075966Z",
     "iopub.status.idle": "2024-11-21T17:20:58.080150Z",
     "shell.execute_reply": "2024-11-21T17:20:58.079330Z"
    },
    "papermill": {
     "duration": 0.014365,
     "end_time": "2024-11-21T17:20:58.081642",
     "exception": false,
     "start_time": "2024-11-21T17:20:58.067277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# def calculate_mean_image_size(image_dir):\n",
    "#     widths = []\n",
    "#     heights = []\n",
    "\n",
    "#     for filename in os.listdir(image_dir):\n",
    "#         if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "#             img_path = os.path.join(image_dir, filename)\n",
    "#             image = cv2.imread(img_path)\n",
    "#             if image is not None:\n",
    "#                 h, w = image.shape[:2]\n",
    "#                 heights.append(h)\n",
    "#                 widths.append(w)\n",
    "#             else:\n",
    "#                 print(f\"Warning: Unable to read image {img_path}\")\n",
    "\n",
    "#     mean_width = int(np.mean(widths))\n",
    "#     mean_height = int(np.mean(heights))\n",
    "#     median_width = int(np.median(widths))\n",
    "#     median_height = int(np.median(heights))\n",
    "\n",
    "#     print(f\"Number of images: {len(widths)}\")\n",
    "#     print(f\"Mean Width: {mean_width}, Mean Height: {mean_height}\")\n",
    "#     print(f\"Median Width: {median_width}, Median Height: {median_height}\")\n",
    "\n",
    "#     return mean_width, mean_height, widths, heights\n",
    "\n",
    "# # Example usage:\n",
    "# image_directory = '/kaggle/input/bkai-igh-neopolyp/test/test'\n",
    "# mean_w, mean_h, widths, heights = calculate_mean_image_size(image_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e0ee490",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:20:58.098219Z",
     "iopub.status.busy": "2024-11-21T17:20:58.097833Z",
     "iopub.status.idle": "2024-11-21T17:20:58.101521Z",
     "shell.execute_reply": "2024-11-21T17:20:58.100778Z"
    },
    "papermill": {
     "duration": 0.013662,
     "end_time": "2024-11-21T17:20:58.103125",
     "exception": false,
     "start_time": "2024-11-21T17:20:58.089463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_dimension_distribution(widths, heights):\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.hist(widths, bins=20, color='skyblue')\n",
    "#     plt.title('Width Distribution')\n",
    "#     plt.xlabel('Width')\n",
    "#     plt.ylabel('Frequency')\n",
    "\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.hist(heights, bins=20, color='salmon')\n",
    "#     plt.title('Height Distribution')\n",
    "#     plt.xlabel('Height')\n",
    "#     plt.ylabel('Frequency')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # After collecting widths and heights in the function above:\n",
    "# plot_dimension_distribution(widths, heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f511cd64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:20:58.160547Z",
     "iopub.status.busy": "2024-11-21T17:20:58.160266Z",
     "iopub.status.idle": "2024-11-21T17:20:58.939878Z",
     "shell.execute_reply": "2024-11-21T17:20:58.939034Z"
    },
    "papermill": {
     "duration": 0.830775,
     "end_time": "2024-11-21T17:20:58.941749",
     "exception": false,
     "start_time": "2024-11-21T17:20:58.110974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_path = []\n",
    "TRAIN_MASK_DIR = '/kaggle/input/bkai-igh-neopolyp/train_gt/train_gt'\n",
    "for root, dirs, files in os.walk(TRAIN_MASK_DIR):\n",
    "    for file in files:\n",
    "        path = os.path.join(root,file)\n",
    "        mask_path.append(path)\n",
    "        \n",
    "len(mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "729e7276",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:20:58.959332Z",
     "iopub.status.busy": "2024-11-21T17:20:58.959083Z",
     "iopub.status.idle": "2024-11-21T17:20:58.968028Z",
     "shell.execute_reply": "2024-11-21T17:20:58.967196Z"
    },
    "papermill": {
     "duration": 0.019518,
     "end_time": "2024-11-21T17:20:58.969682",
     "exception": false,
     "start_time": "2024-11-21T17:20:58.950164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DatasetCustom(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, resize=None, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.resize = resize\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(self.img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def read_mask(self, mask_path):\n",
    "        image = cv2.imread(mask_path)\n",
    "        image = cv2.resize(image, self.resize, interpolation=cv2.INTER_AREA)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        lower_red1 = np.array([0, 100, 20])\n",
    "        upper_red1 = np.array([10, 255, 255])\n",
    "        lower_red2 = np.array([160,100,20])\n",
    "        upper_red2 = np.array([179,255,255])\n",
    "        \n",
    "        lower_mask_red = cv2.inRange(image, lower_red1, upper_red1)\n",
    "        upper_mask_red = cv2.inRange(image, lower_red2, upper_red2)\n",
    "        \n",
    "        red_mask = lower_mask_red + upper_mask_red\n",
    "        red_mask[red_mask != 0] = 1\n",
    "\n",
    "        green_mask = cv2.inRange(image, (36, 25, 25), (70, 255, 255))\n",
    "        green_mask[green_mask != 0] = 2\n",
    "\n",
    "        full_mask = cv2.bitwise_or(red_mask, green_mask)\n",
    "        full_mask = np.expand_dims(full_mask, axis=-1) \n",
    "        full_mask = full_mask.astype(np.uint8)\n",
    "        \n",
    "        return full_mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.images[idx])\n",
    "        image = cv2.imread(img_path)  #  BGR\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Convert to RGB\n",
    "        label = self.read_mask(label_path)  \n",
    "        image = cv2.resize(image, self.resize, interpolation=cv2.INTER_AREA)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8bf3219",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:20:58.986750Z",
     "iopub.status.busy": "2024-11-21T17:20:58.986527Z",
     "iopub.status.idle": "2024-11-21T17:20:58.990021Z",
     "shell.execute_reply": "2024-11-21T17:20:58.989441Z"
    },
    "papermill": {
     "duration": 0.013853,
     "end_time": "2024-11-21T17:20:58.991536",
     "exception": false,
     "start_time": "2024-11-21T17:20:58.977683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_resize = (512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "974d7f06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:20:59.008040Z",
     "iopub.status.busy": "2024-11-21T17:20:59.007818Z",
     "iopub.status.idle": "2024-11-21T17:20:59.011905Z",
     "shell.execute_reply": "2024-11-21T17:20:59.011146Z"
    },
    "papermill": {
     "duration": 0.013948,
     "end_time": "2024-11-21T17:20:59.013418",
     "exception": false,
     "start_time": "2024-11-21T17:20:58.999470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = DatasetCustom(img_dir= TRAIN_DIR,\n",
    "                             label_dir= TRAIN_MASK_DIR,\n",
    "                             resize= img_resize,\n",
    "                             transform = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099874d3",
   "metadata": {
    "papermill": {
     "duration": 0.007625,
     "end_time": "2024-11-21T17:20:59.028960",
     "exception": false,
     "start_time": "2024-11-21T17:20:59.021335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a17d8722",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:20:59.045398Z",
     "iopub.status.busy": "2024-11-21T17:20:59.045162Z",
     "iopub.status.idle": "2024-11-21T17:20:59.050380Z",
     "shell.execute_reply": "2024-11-21T17:20:59.049677Z"
    },
    "papermill": {
     "duration": 0.015238,
     "end_time": "2024-11-21T17:20:59.051942",
     "exception": false,
     "start_time": "2024-11-21T17:20:59.036704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AugmentDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None, length_multiplier=1):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.length_multiplier = length_multiplier\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = idx % len(self.dataset)\n",
    "        image, label = self.dataset[actual_idx]\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=label)\n",
    "            image = transformed['image'].float()\n",
    "            label = transformed['mask'].float()\n",
    "            label = label.permute(2, 0, 1)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) * self.length_multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef767871",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:20:59.068423Z",
     "iopub.status.busy": "2024-11-21T17:20:59.068195Z",
     "iopub.status.idle": "2024-11-21T17:20:59.073833Z",
     "shell.execute_reply": "2024-11-21T17:20:59.073194Z"
    },
    "papermill": {
     "duration": 0.015745,
     "end_time": "2024-11-21T17:20:59.075406",
     "exception": false,
     "start_time": "2024-11-21T17:20:59.059661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transformation = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomGamma (gamma_limit=(70, 130), eps=None, always_apply=False, p=0.2),\n",
    "    A.RGBShift(p=0.3, r_shift_limit=10, g_shift_limit=10, b_shift_limit=10),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transformation = A.Compose([\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ea99bf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:20:59.092047Z",
     "iopub.status.busy": "2024-11-21T17:20:59.091784Z",
     "iopub.status.idle": "2024-11-21T17:20:59.102253Z",
     "shell.execute_reply": "2024-11-21T17:20:59.101705Z"
    },
    "papermill": {
     "duration": 0.020448,
     "end_time": "2024-11-21T17:20:59.103761",
     "exception": false,
     "start_time": "2024-11-21T17:20:59.083313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the split sizes\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 8\n",
    "# Randomly split the dataset\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "train_dataset = AugmentDataset(train_dataset, transform=train_transformation, length_multiplier=1)\n",
    "val_dataset = AugmentDataset(val_dataset, transform=val_transformation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8470f61f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:20:59.120264Z",
     "iopub.status.busy": "2024-11-21T17:20:59.120035Z",
     "iopub.status.idle": "2024-11-21T17:20:59.124698Z",
     "shell.execute_reply": "2024-11-21T17:20:59.124009Z"
    },
    "papermill": {
     "duration": 0.014812,
     "end_time": "2024-11-21T17:20:59.126274",
     "exception": false,
     "start_time": "2024-11-21T17:20:59.111462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d7850",
   "metadata": {
    "papermill": {
     "duration": 0.007827,
     "end_time": "2024-11-21T17:20:59.141977",
     "exception": false,
     "start_time": "2024-11-21T17:20:59.134150",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9c29cb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:20:59.159071Z",
     "iopub.status.busy": "2024-11-21T17:20:59.158343Z",
     "iopub.status.idle": "2024-11-21T17:21:00.175787Z",
     "shell.execute_reply": "2024-11-21T17:21:00.174846Z"
    },
    "papermill": {
     "duration": 1.028167,
     "end_time": "2024-11-21T17:21:00.177940",
     "exception": false,
     "start_time": "2024-11-21T17:20:59.149773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83.3M/83.3M [00:00<00:00, 314MB/s]\n"
     ]
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=\"resnet34\",        \n",
    "    encoder_weights=\"imagenet\",     \n",
    "    in_channels=3,                  \n",
    "    classes=3     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "365abb8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:21:00.196372Z",
     "iopub.status.busy": "2024-11-21T17:21:00.196094Z",
     "iopub.status.idle": "2024-11-21T17:21:00.199668Z",
     "shell.execute_reply": "2024-11-21T17:21:00.198910Z"
    },
    "papermill": {
     "duration": 0.014905,
     "end_time": "2024-11-21T17:21:00.201701",
     "exception": false,
     "start_time": "2024-11-21T17:21:00.186796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('/kaggle/input/kaggle-notebook/model.pth')\n",
    "# model.load_state_dict(checkpoint['model'])\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "775c5b6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:21:00.226964Z",
     "iopub.status.busy": "2024-11-21T17:21:00.226336Z",
     "iopub.status.idle": "2024-11-21T17:21:00.231876Z",
     "shell.execute_reply": "2024-11-21T17:21:00.230997Z"
    },
    "papermill": {
     "duration": 0.022535,
     "end_time": "2024-11-21T17:21:00.233581",
     "exception": false,
     "start_time": "2024-11-21T17:21:00.211046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "color_dict= {0: (0, 0, 0),\n",
    "             1: (255, 0, 0),\n",
    "             2: (0, 255, 0)}\n",
    "def mask_to_rgb(mask, color_dict):\n",
    "    output = np.zeros((mask.shape[0], mask.shape[1], 3))\n",
    "\n",
    "    for k in color_dict.keys():\n",
    "        output[mask==k] = color_dict[k]\n",
    "\n",
    "    return np.uint8(output)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01a7dd10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:21:00.255410Z",
     "iopub.status.busy": "2024-11-21T17:21:00.254830Z",
     "iopub.status.idle": "2024-11-21T17:21:00.258935Z",
     "shell.execute_reply": "2024-11-21T17:21:00.258154Z"
    },
    "papermill": {
     "duration": 0.015082,
     "end_time": "2024-11-21T17:21:00.260581",
     "exception": false,
     "start_time": "2024-11-21T17:21:00.245499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del dataset\n",
    "del train_dataset\n",
    "del val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1edfad2",
   "metadata": {
    "papermill": {
     "duration": 0.008084,
     "end_time": "2024-11-21T17:21:00.276960",
     "exception": false,
     "start_time": "2024-11-21T17:21:00.268876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d15f0eb",
   "metadata": {
    "papermill": {
     "duration": 0.008068,
     "end_time": "2024-11-21T17:21:00.293255",
     "exception": false,
     "start_time": "2024-11-21T17:21:00.285187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Define Dice loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bd10f39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:21:00.310872Z",
     "iopub.status.busy": "2024-11-21T17:21:00.310538Z",
     "iopub.status.idle": "2024-11-21T17:21:00.319465Z",
     "shell.execute_reply": "2024-11-21T17:21:00.318698Z"
    },
    "papermill": {
     "duration": 0.019649,
     "end_time": "2024-11-21T17:21:00.321120",
     "exception": false,
     "start_time": "2024-11-21T17:21:00.301471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, num_classes=3, epsilon=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        outputs = F.softmax(outputs, dim=1)\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=self.num_classes).permute(0, 3, 1, 2).float()\n",
    "\n",
    "        # Flatten the tensors\n",
    "        outputs_flat = outputs.contiguous().view(-1, self.num_classes)\n",
    "        targets_flat = targets_one_hot.contiguous().view(-1, self.num_classes)\n",
    "\n",
    "        intersection = torch.sum(outputs_flat * targets_flat, dim=0)\n",
    "        union = torch.sum(outputs_flat + targets_flat, dim=0)\n",
    "\n",
    "        dice_loss = 1 - (2 * intersection + self.epsilon) / (union + self.epsilon)\n",
    "        mean_dice_loss = dice_loss.mean()\n",
    "\n",
    "        return mean_dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2c0ff08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:21:00.347629Z",
     "iopub.status.busy": "2024-11-21T17:21:00.347032Z",
     "iopub.status.idle": "2024-11-21T17:21:00.577200Z",
     "shell.execute_reply": "2024-11-21T17:21:00.576276Z"
    },
    "papermill": {
     "duration": 0.249437,
     "end_time": "2024-11-21T17:21:00.579824",
     "exception": false,
     "start_time": "2024-11-21T17:21:00.330387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model.to(device)\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "criterion_dice = DiceLoss(num_classes=3)\n",
    "alpha = 0.5  # Weight for balancing the two losses\n",
    "best_val_loss = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f39e72a",
   "metadata": {
    "papermill": {
     "duration": 0.008259,
     "end_time": "2024-11-21T17:21:00.602158",
     "exception": false,
     "start_time": "2024-11-21T17:21:00.593899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Make use of learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "859647ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:21:00.620551Z",
     "iopub.status.busy": "2024-11-21T17:21:00.619739Z",
     "iopub.status.idle": "2024-11-21T17:21:00.627733Z",
     "shell.execute_reply": "2024-11-21T17:21:00.627009Z"
    },
    "papermill": {
     "duration": 0.019011,
     "end_time": "2024-11-21T17:21:00.629393",
     "exception": false,
     "start_time": "2024-11-21T17:21:00.610382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Assuming you've already defined your optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize the scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min',                 # We want to minimize the validation loss\n",
    "    factor=0.5,                 # Reduce LR by a factor of 0.5\n",
    "    patience=1,                 # Number of epochs with no improvement after which LR will be reduced\n",
    "    verbose=True,               # Print a message when LR is updated\n",
    "    threshold=0.0001,           # Threshold for measuring the new optimum\n",
    "    threshold_mode='rel',       # Mode for threshold\n",
    "    cooldown=0,                 # Number of epochs to wait before resuming normal operation after LR has been reduced\n",
    "    min_lr=1e-6                 # Lower bound on the learning rate\n",
    ")\n",
    "\n",
    "# scheduler2 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4613f3fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:21:00.647467Z",
     "iopub.status.busy": "2024-11-21T17:21:00.646936Z",
     "iopub.status.idle": "2024-11-21T17:21:30.997955Z",
     "shell.execute_reply": "2024-11-21T17:21:30.996985Z"
    },
    "papermill": {
     "duration": 30.362273,
     "end_time": "2024-11-21T17:21:30.999951",
     "exception": false,
     "start_time": "2024-11-21T17:21:00.637678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtu7pham7\u001b[0m (\u001b[33mhustcollab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20241121_172102-a7jwzrey\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mazure-bush-38\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/hustcollab/PolypSegment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/hustcollab/PolypSegment/runs/a7jwzrey\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(\n",
    "    # set the wandb project where this run will be logged\n",
    "#     project= \"PolypSegment\", \n",
    "    key = \"e02f7703b40a2b3e0ab4801d4cb1d86b3b7327a6\",\n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "    project=\"PolypSegment\",\n",
    "    config={\n",
    "        \"init_learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"loss_functions\": [\"CrossEntropy\", \"Dice\"],\n",
    "        \"alpha\": alpha\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add the model to WandB\n",
    "wandb.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4841b8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T17:21:31.020603Z",
     "iopub.status.busy": "2024-11-21T17:21:31.020282Z",
     "iopub.status.idle": "2024-11-21T18:13:57.453321Z",
     "shell.execute_reply": "2024-11-21T18:13:57.452371Z"
    },
    "papermill": {
     "duration": 3146.460576,
     "end_time": "2024-11-21T18:13:57.470004",
     "exception": false,
     "start_time": "2024-11-21T17:21:31.009428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:   0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35], Loss: 0.0663998076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:   3%|â–         | 1/35 [01:38<55:52, 98.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/35], Loss: 0.0486318148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:   6%|â–Œ         | 2/35 [03:09<51:34, 93.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/35], Loss: 0.0389242474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  11%|â–ˆâ–        | 4/35 [06:08<47:06, 91.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/35], Loss: 0.0390543661\n",
      "Epoch [5/35], Loss: 0.0350760563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  14%|â–ˆâ–        | 5/35 [07:39<45:23, 90.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/35], Loss: 0.0314393639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  20%|â–ˆâ–ˆ        | 7/35 [10:37<41:58, 89.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/35], Loss: 0.0347791351\n",
      "Epoch [8/35], Loss: 0.0305885218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  23%|â–ˆâ–ˆâ–       | 8/35 [12:07<40:30, 90.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/35], Loss: 0.0305177118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  26%|â–ˆâ–ˆâ–Œ       | 9/35 [13:37<38:59, 89.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/35], Loss: 0.0266825544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [16:37<35:57, 89.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/35], Loss: 0.0300649859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [18:07<34:24, 89.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/35], Loss: 0.0323929200\n",
      "Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch [13/35], Loss: 0.0254467883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [19:36<32:53, 89.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/35], Loss: 0.0237627656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 15/35 [22:34<29:43, 89.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/35], Loss: 0.0267392872\n",
      "Epoch [16/35], Loss: 0.0233724514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [24:04<28:19, 89.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/35], Loss: 0.0220998698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [27:03<25:24, 89.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/35], Loss: 0.0223698194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [28:33<23:52, 89.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/35], Loss: 0.0227389991\n",
      "Epoch 00019: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch [20/35], Loss: 0.0199677825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [30:03<22:24, 89.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/35], Loss: 0.0193628809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [31:32<20:55, 89.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/35], Loss: 0.0187667038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23/35 [34:31<17:53, 89.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/35], Loss: 0.0191153316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [36:00<16:23, 89.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/35], Loss: 0.0201555043\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch [25/35], Loss: 0.0178032411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [38:58<13:23, 89.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/35], Loss: 0.0185487759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 27/35 [40:27<11:52, 89.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/35], Loss: 0.0178188355\n",
      "Epoch 00027: reducing learning rate of group 0 to 6.2500e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [41:56<10:23, 89.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/35], Loss: 0.0183827351\n",
      "Epoch [29/35], Loss: 0.0177172243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [44:56<07:27, 89.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/35], Loss: 0.0179020925\n",
      "Epoch [31/35], Loss: 0.0175529104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [47:56<04:29, 89.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/35], Loss: 0.0180320792\n",
      "Epoch [33/35], Loss: 0.0172105301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/35 [49:26<02:59, 89.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/35], Loss: 0.0164771957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [52:26<00:00, 89.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/35], Loss: 0.0174445825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 35\n",
    "epoch_bar = tqdm(total=num_epochs, desc='Total Progress')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        labels = labels.squeeze(dim=1).long()\n",
    "        outputs = model(images)\n",
    "    \n",
    "        loss_ce = criterion_ce(outputs, labels)\n",
    "        loss_dice = criterion_dice(outputs, labels)\n",
    "        loss = alpha * loss_ce + (1 - alpha) * loss_dice\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "#         del images, labels, outputs, loss\n",
    "#         torch.cuda.empty_cache()\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels = labels.squeeze(dim=1).long()\n",
    "            \n",
    "            outputs = model(images)\n",
    "\n",
    "            val_loss += criterion_dice(outputs.float(),labels.long()).item()\n",
    "            \n",
    "#             del images, labels, outputs\n",
    "#             torch.cuda.empty_cache()\n",
    "            \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {val_loss/len(val_loader):.10f}\")\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        checkpoint = { \n",
    "            'epoch': epoch,\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'loss': val_loss,\n",
    "        }\n",
    "        save_path = f'model.pth'\n",
    "        torch.save(checkpoint, save_path)\n",
    "        \n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    epoch_bar.update(1)\n",
    "    wandb.log({'Val_loss': val_loss/len(val_loader),'Train_loss': train_loss/len(train_loader), 'Learning_rate' : current_lr})\n",
    "epoch_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9201b52c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T18:13:57.497316Z",
     "iopub.status.busy": "2024-11-21T18:13:57.497037Z",
     "iopub.status.idle": "2024-11-21T18:13:57.729083Z",
     "shell.execute_reply": "2024-11-21T18:13:57.728225Z"
    },
    "papermill": {
     "duration": 0.248151,
     "end_time": "2024-11-21T18:13:57.730952",
     "exception": false,
     "start_time": "2024-11-21T18:13:57.482801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnetPlusPlus(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): UnetPlusPlusDecoder(\n",
       "    (center): Identity()\n",
       "    (blocks): ModuleDict(\n",
       "      (x_0_0): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_0_1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_1_1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_0_2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_1_2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_2_2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_0_3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(320, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_1_3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_2_3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_3_3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (x_0_4): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = smp.UnetPlusPlus(\n",
    "#     encoder_name=\"resnet34\",        \n",
    "#     encoder_weights=\"imagenet\",     \n",
    "#     in_channels=3,                  \n",
    "#     classes=3     \n",
    "# )\n",
    "checkpoint = torch.load('/kaggle/working/model.pth')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43a9ec9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T18:13:57.758091Z",
     "iopub.status.busy": "2024-11-21T18:13:57.757793Z",
     "iopub.status.idle": "2024-11-21T18:13:58.780326Z",
     "shell.execute_reply": "2024-11-21T18:13:58.779073Z"
    },
    "papermill": {
     "duration": 1.038072,
     "end_time": "2024-11-21T18:13:58.782389",
     "exception": false,
     "start_time": "2024-11-21T18:13:57.744317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bf1245",
   "metadata": {
    "papermill": {
     "duration": 0.012274,
     "end_time": "2024-11-21T18:13:58.808007",
     "exception": false,
     "start_time": "2024-11-21T18:13:58.795733",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Get Testset Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "befc9b79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T18:13:58.834687Z",
     "iopub.status.busy": "2024-11-21T18:13:58.834360Z",
     "iopub.status.idle": "2024-11-21T18:14:27.005747Z",
     "shell.execute_reply": "2024-11-21T18:14:27.004815Z"
    },
    "papermill": {
     "duration": 28.186917,
     "end_time": "2024-11-21T18:14:27.007879",
     "exception": false,
     "start_time": "2024-11-21T18:13:58.820962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for i in os.listdir(\"/kaggle/input/bkai-igh-neopolyp/test/test\"):\n",
    "    img_path = os.path.join(\"/kaggle/input/bkai-igh-neopolyp/test/test\", i)\n",
    "    ori_img = cv2.imread(img_path)\n",
    "    ori_img = cv2.cvtColor(ori_img, cv2.COLOR_BGR2RGB)\n",
    "    ori_h = ori_img.shape[0]\n",
    "    ori_w = ori_img.shape[1]\n",
    "    img = cv2.resize(ori_img, img_resize, interpolation=cv2.INTER_AREA)\n",
    "    transformed = val_transformation(image=img)\n",
    "    input_img = transformed[\"image\"]\n",
    "    input_img = input_img.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output_mask = model(input_img).squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "    mask = cv2.resize(output_mask, (ori_w, ori_h), interpolation=cv2.INTER_CUBIC)\n",
    "    mask = np.argmax(mask, axis=2)\n",
    "    mask_rgb = mask_to_rgb(mask, color_dict)\n",
    "    mask_rgb = cv2.cvtColor(mask_rgb, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(\"prediction/{}\".format(i), mask_rgb)\n",
    "    # Clear variables to free memory\n",
    "    del img, input_img, output_mask, mask, mask_rgb\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a485cb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T18:14:27.034720Z",
     "iopub.status.busy": "2024-11-21T18:14:27.034432Z",
     "iopub.status.idle": "2024-11-21T18:14:29.866354Z",
     "shell.execute_reply": "2024-11-21T18:14:29.865689Z"
    },
    "papermill": {
     "duration": 2.847374,
     "end_time": "2024-11-21T18:14:29.868358",
     "exception": false,
     "start_time": "2024-11-21T18:14:27.020984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rle_to_string(runs):\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_encode_one_mask(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels[pixels > 225] = 255\n",
    "    pixels[pixels <= 225] = 0\n",
    "    use_padding = False\n",
    "    if pixels[0] or pixels[-1]:\n",
    "        use_padding = True\n",
    "        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n",
    "        pixel_padded[1:-1] = pixels\n",
    "        pixels = pixel_padded\n",
    "    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    if use_padding:\n",
    "        rle = rle - 1\n",
    "    rle[1::2] = rle[1::2] - rle[:-1:2]\n",
    "    \n",
    "    return rle_to_string(rle)\n",
    "\n",
    "def rle2mask(mask_rle, shape=(3,3)):\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T\n",
    "\n",
    "def mask2string(dir):\n",
    "    strings = []\n",
    "    ids = []\n",
    "    ws, hs = [[] for i in range(2)]\n",
    "    for image_id in os.listdir(dir):\n",
    "        id = image_id.split('.')[0]\n",
    "        path = os.path.join(dir, image_id)\n",
    "        # print(path)\n",
    "        img = cv2.imread(path)[:,:,::-1]\n",
    "        h, w = img.shape[0], img.shape[1]\n",
    "        for channel in range(2):\n",
    "            ws.append(w)\n",
    "            hs.append(h)\n",
    "            ids.append(f'{id}_{channel}')\n",
    "            string = rle_encode_one_mask(img[:,:,channel])\n",
    "            strings.append(string)\n",
    "    r = {\n",
    "        'ids': ids,\n",
    "        'strings': strings,\n",
    "    }\n",
    "    return r\n",
    "\n",
    "\n",
    "MASK_DIR_PATH = '/kaggle/working/prediction'\n",
    "dir = MASK_DIR_PATH\n",
    "res = mask2string(dir)\n",
    "df = pd.DataFrame(columns=['Id', 'Expected'])\n",
    "df['Id'] = res['ids']\n",
    "df['Expected'] = res['strings']\n",
    "\n",
    "df.to_csv(r'output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d81d08b",
   "metadata": {
    "papermill": {
     "duration": 0.012142,
     "end_time": "2024-11-21T18:14:29.893606",
     "exception": false,
     "start_time": "2024-11-21T18:14:29.881464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bc9767e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T18:14:29.919702Z",
     "iopub.status.busy": "2024-11-21T18:14:29.919445Z",
     "iopub.status.idle": "2024-11-21T18:14:29.922940Z",
     "shell.execute_reply": "2024-11-21T18:14:29.922149Z"
    },
    "papermill": {
     "duration": 0.018596,
     "end_time": "2024-11-21T18:14:29.924522",
     "exception": false,
     "start_time": "2024-11-21T18:14:29.905926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torch.jit import load\n",
    "# model = UNet()\n",
    "# optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# checkpoint = torch.load(pretrained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a705076",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T18:14:29.950272Z",
     "iopub.status.busy": "2024-11-21T18:14:29.950057Z",
     "iopub.status.idle": "2024-11-21T18:14:29.953306Z",
     "shell.execute_reply": "2024-11-21T18:14:29.952714Z"
    },
    "papermill": {
     "duration": 0.01794,
     "end_time": "2024-11-21T18:14:29.954887",
     "exception": false,
     "start_time": "2024-11-21T18:14:29.936947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74b98cfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T18:14:29.981159Z",
     "iopub.status.busy": "2024-11-21T18:14:29.980951Z",
     "iopub.status.idle": "2024-11-21T18:14:29.984431Z",
     "shell.execute_reply": "2024-11-21T18:14:29.983406Z"
    },
    "papermill": {
     "duration": 0.01842,
     "end_time": "2024-11-21T18:14:29.986093",
     "exception": false,
     "start_time": "2024-11-21T18:14:29.967673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from collections import OrderedDict\n",
    "# new_state_dict = OrderedDict()\n",
    "# for k, v in checkpoint['model'].items():\n",
    "#     name = k[7:] # remove `module.`\n",
    "#     new_state_dict[name] = v\n",
    "# # load params\n",
    "# model.load_state_dict(new_state_dict)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 2715462,
     "sourceId": 30892,
     "sourceType": "competition"
    },
    {
     "sourceId": 208797485,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30580,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3260.304135,
   "end_time": "2024-11-21T18:14:32.720132",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-21T17:20:12.415997",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
